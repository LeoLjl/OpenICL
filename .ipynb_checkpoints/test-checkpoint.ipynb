{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac39377-d1b8-400d-9724-ea1624604a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-04 15:04:29.760138: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-04 15:04:30.544895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from openicl import DatasetReader\n",
    "\n",
    "# Loading dataset from huggingface\n",
    "dataset = load_dataset('SetFit/sst5')\n",
    "\n",
    "# Define a DatasetReader, with specified column names where input and output are stored.\n",
    "data = DatasetReader(dataset, input_columns=['text'], output_column='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a058db-416f-487d-b2fa-d25b5053f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openicl import PromptTemplate\n",
    "\n",
    "tp_dict = {\n",
    "    0: \"</E>Review: <X>\\nSentiment: terrible\",\n",
    "    1: \"</E>Review: <X>\\nSentiment: bad\",\n",
    "    2: \"</E>Review: <X>\\nSentiment: okay\",\n",
    "    3: \"</E>Review: <X>\\nSentiment: good\",\n",
    "    4: \"</E>Review: <X>\\nSentiment: great\",\n",
    "}\n",
    "template = PromptTemplate(tp_dict, column_token_map={'text' : '<X>'}, ice_token='</E>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e300896-4e63-4eae-9b53-01a453f9959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openicl import TopkRetriever\n",
    "# Define a retriever using the previous `DataLoader`.\n",
    "# `ice_num` stands for the number of data in in-context examples.\n",
    "retriever = TopkRetriever(data, ice_num=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b66cc6c-0d72-408d-b956-02dc4e4bc19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openicl import GenInferencer\n",
    "\n",
    "inferencer = GenInferencer(model_name='distilgpt2', batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e7ebe7-723e-4feb-b342-137ab08bb4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-04 14:13:16,625] [openicl.icl_retriever.icl_topk_retriever] [INFO] Embedding test set...\n",
      "  0%|          | 0/2210 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 2210/2210 [00:31<00:00, 70.83it/s]\n",
      "[2023-08-04 14:13:47,844] [openicl.icl_retriever.icl_topk_retriever] [INFO] Retrieving data for test set...\n",
      "100%|██████████| 2210/2210 [00:18<00:00, 117.07it/s]\n",
      "[2023-08-04 14:14:08,360] [openicl.icl_inferencer.icl_gen_inferencer] [INFO] Starting inference process...\n",
      "  0%|          | 0/185 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [02:28<00:00,  1.24it/s]\n",
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 1.21MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from openicl import AccEvaluator\n",
    "# the inferencer requires retriever to collect in-context examples, as well as a template to wrap up these examples.\n",
    "predictions = inferencer.inference(retriever, ice_template=template)\n",
    "# compute accuracy for the prediction\n",
    "score = AccEvaluator().score(predictions=predictions, references=data.references)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f6ec7-a807-4e2d-b5cc-30725604b9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
